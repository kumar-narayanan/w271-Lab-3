---
title: 'W271-Summer 2022, Lab 3: Panel Models'
subtitle: 'US Traffic Fatalities: 1980 - 2004'
author: "Finnian Meagher, Sandeep Kataria, Satheesh Joseph, Kumar Narayanan"
output: 
  bookdown::pdf_document2: default
---

```{r load packages, echo=FALSE, warning=FALSE, message=FALSE, inlcude=FALSE}
install.packages("broom", repos = "http://cran.us.r-project.org")
library(broom)
if(!"car"%in%rownames(installed.packages())) {install.packages("car")} 
library(car)

install.packages("corrplot")
library(corrplot)

library(data.table)
library(dplyr)
library(fable )
library(feasts)
library(forecast)
library(fpp3)

install.packages("GGally")
library(GGally)

library(ggplot2)

install.packages("ggrepel")
library(ggrepel)

library(gridExtra)
library(gtools)

install.packages("Hmisc")
library(Hmisc)

library(knitr)
library(lubridate)
library(patchwork)
library(performance)

install.packages("plm")
library(plm)

library(plyr)
library(stats)

install.packages("stargazer")
library(stargazer)

library(tidyr)
library(tidyverse)
library(tseries)
library(tsibble)

install.packages("vcd")
library(vcd)

library(xtable)

if(!"ggthemes"%in%rownames(installed.packages())) {install.packages("ggthemes")} 
library(ggthemes)
```

```{r set themes}
theme_set(theme_minimal())
```

# U.S. traffic fatalities: 1980-2004

In this lab, we are asking you to answer the following **causal** question: 

> **"Do changes in traffic laws affect traffic fatalities?"**  

```{r load data, echo = TRUE, warning=FALSE}
load(file="./data/driving.RData")

# quick view of the data and its parameters
# glimpse(data)
desc

# get the column names in character format
desc$label <- as.character(desc$label)

# save the data into a data frame
traffic_df <- data
```


# (30 points, total) Build and Describe the Data 

1. (5 points) Load the data and produce useful features. Specifically: 
    - Produce a new variable, called `speed_limit` that re-encodes the data that is in `sl55`, `sl65`, `sl70`, `sl75`, and `slnone`; 
    - Produce a new variable, called `year_of_observation` that re-encodes the data that is in `d80`, `d81`, ... , `d04`. 
    - Produce a new variable for each of the other variables that are one-hot encoded (i.e. `bac*` variable series). 
    - Rename these variables to sensible names that are legible to a reader of your analysis. For example, the dependent variable as provided is called, `totfatrte`. Pick something more sensible, like, `total_fatalities_rate`. There are few enough of these variables to change, that you should change them for all the variables in the data. (You will thank yourself later.)
    
```{r basic eda}
## variable names
# names(traffic_df)

# dimension of the data frame
dim(traffic_df)

# check if the panel data is balanced
traff_table <- traffic_df %>% dplyr::select(year, state) %>% table()

# compute the row sums from the table above
val <- rowSums(traff_table)
# val

unbalanced_data <- 0

# in a loop check if any of the value is less than 48 (num. unique states)
for (i in val) {
  if (i != length(unique(traffic_df$state))) {
    state_code <- states_df[states_df$index == i, ]$state
    cat(paste("State", state_code, " at index", i, "is not balanced"))
    cat("------------------------\n")
    unbalanced_data <- unbalanced_data + 1
  }
}

if (unbalanced_data) {
  cat(pastes("-There are ", unbalanced_data, "states with unbalanced data-\n"))
} else {
  cat("-------The data set is balanced-------\n")
}

# check for gaps in the time series of each state
traffic_df %>% is.pconsecutive(index=c("state", "year"))

if (sum(traffic_df %>% is.pconsecutive(index=c("state", "year")) - TRUE)) {
  cat("-----Gaps exist in data; check for gaps-----\n")
} else {
  cat("-----There are no gaps in the data------\n")
}
```

```{r eda round 2}
# new variable, speed_limit; re-encode columns sl55, sl60, sl65, sl70, sl75, slnone
traffic_df$speed_limit <- ifelse(traffic_df$sl55 >= 0.5, 55,
                              ifelse(traffic_df$sl60 >= 0.5, 60,
                                  ifelse(traffic_df$sl65 >= 0.5, 65,
                                      ifelse(traffic_df$sl70 >= 0.5, 70,
                                          ifelse(traffic_df$sl75 >= 0.5, 75, 0
                                                 )))))

# new variable, year_of_observation; re-encode columns d80, ..., d04
traffic_df$year_of_observation <- ifelse(traffic_df$d80 == 1, 1980,
                                  ifelse(traffic_df$d81 == 1, 1981,
                                  ifelse(traffic_df$d82 == 1, 1982,
                                  ifelse(traffic_df$d83 == 1, 1983,
                                  ifelse(traffic_df$d84 == 1, 1984,
                                  ifelse(traffic_df$d85 == 1, 1985,
                                  ifelse(traffic_df$d86 == 1, 1986,
                                  ifelse(traffic_df$d87 == 1, 1987,
                                  ifelse(traffic_df$d88 == 1, 1988,
                                  ifelse(traffic_df$d89 == 1, 1989,
                                  ifelse(traffic_df$d90 == 1, 1990,
                                  ifelse(traffic_df$d91 == 1, 1991,
                                  ifelse(traffic_df$d92 == 1, 1992,
                                  ifelse(traffic_df$d93 == 1, 1993,
                                  ifelse(traffic_df$d94 == 1, 1994,
                                  ifelse(traffic_df$d95 == 1, 1995,
                                  ifelse(traffic_df$d96 == 1, 1996,
                                  ifelse(traffic_df$d97 == 1, 1997,
                                  ifelse(traffic_df$d98 == 1, 1998,
                                  ifelse(traffic_df$d99 == 1, 1999,
                                  ifelse(traffic_df$d00 == 1, 2000,
                                  ifelse(traffic_df$d01 == 1, 2001,
                                  ifelse(traffic_df$d02 == 1, 2002,
                                  ifelse(traffic_df$d03 == 1, 2003, 
                                         2004))))))))))))))))))))))))

# new variable for each of the other one-hot encoded variables (bac* variables)
unique(traffic_df$bac10)
traffic_df$blood_alc_lim_10 <- ifelse(traffic_df$bac10 >= 0.5, 1, 0)
unique(traffic_df$bac08)
traffic_df$blood_alc_lim_08 <- ifelse(traffic_df$bac08 >= 0.5, 1, 0)

# rename columns to reflect a meaningful description of the column
traffic_df <- traffic_df %>% rename( c(
  "gdl" = "grad_drivers_lic_law",
  "perse"= "adm_lic_revoc_law",
  "totfat" = "total_fatalities",
  "nghtfat" = "night_fatalities",
  "wkndfat" = "weekend_fatalities",
  "totfatpvm" = "total_fatal_per_100mm",
  "nghtfatpvm" = "night_fatal_per_100mm" ,
  "wkndfatpvm" = "weekend_fatal_per_100mm",
  "statepop" = "state_population",
  "totfatrte" = "total_fatality_rate",
  "nghtfatrte" = "night_fatality_rate",
  "wkndfatrte" = "weekend_fatality_rate",
  "vehicmiles" = "vehicle_miles_traveled_billions",
  "unem" = "unemployment_rate",
  "sbprim" = "primary_seatbelt_law",
  "sbsecon" = "secondary_seatbelt_law",
  "perc14_24" = "percent_pop_aged_14_to_24",
  "vehicmilespc" = "miles_driven_per_capita")
  )

# add a column for the 2 letter state code
states_2ltr_code <- sort(c(state.abb,"DC"))
state_code = c()
for (i in traffic_df$state) {
  state_code = c(state_code, states_2ltr_code[i])
}
traffic_df$state_code <- state_code

colnames(traffic_df)

# convert data frame to pdata.frame
traffic_pdf <- pdata.frame(traffic_df, index=c("state", "year"))

## Check the structure of panel data
pdim(traffic_pdf)

# head(traffic_pdf, 10)
```

2. (5 points) Provide a description of the basic structure of the dataset. What is this data? How, where, and when is it collected? Is the data generated through a survey or some other method? Is the data that is presented a sample from the population, or is it a *census* that represents the entire population? Minimally, this should include:
    - How is the our dependent variable of interest `total_fatalities_rate` defined? 
    
**The data set that we plan to analyze is what is called Panel Data. The data set contains observations for every state (in the continental USA, which is made of 48 states in the contiguous land, except Delaware, and including Washington DC) from 1980 to 2004 (25 years) of vehicular accident data.**

**The data is organized as 25 rows (one for each calendar year) for every state. The total number of rows is** `r nrow(traffic_df)` **which is 48 states times 25 years. The sequence for each states is from 1980 to 2004. All the data for a given state is listed for all 25 years before the next state is taken up. The states are organized in alphabetical order.**

## Fatalities Information
**Primarily, for each year and for every state the data set measures various fatalities (total fatalities, night fatalities, weekend fatalities, and their respective rates by for every 100,000 people in the state and per every 100 Million Mile driven). The data contains several driving laws for each state (such as speed limits, blood alcohol content level for declaring driver being intoxicated, minimum driving age, zero-tolerence policy etc.). There is one column "perse" in the original data set (that we have renamed it as "adm_lic_revoc_law"), that tells if the state can revoke license without a trial, and seat belt laws. There are also demographic details (population age) and economic data (unemployment rate).**

**A few state law variables, such as bac08 and bac10 are coded as dichotomous options, i.e. 1 and 0, indicating whether the law was applicable in that state in that year. A fraction value indicates that fraction of the year for which that law was applicable in that state. e.g. a fraction value of 0.667 means that the law was implemented in that state for 2/3 rd of the duration in that year.**

## Demographic Information
**'vehicle_miles_traveled_billions', 'miles_driven_per_capita', 'unemployment_rate' and 'percent_pop_aged_14_to_24' are the four demographic variables. 'vehicle_miles_traveled_billions' and 'miles_driven_per_capita' account for overall miles driven and per capita miles driven by state's population. 'unemployment_rate' represents the rate of unemployment in a state in a given year. 'percent_pop_aged_14_to_24' represents how much % of state's population is youth.**

## Traffic Laws
**There are quite a few variables explaining traffic laws across states over time.**

**Seatbelt related laws, i.e. 'primary_seatbelt_law' and 'secondary_seatbelt_law' are categorical variables with '0' indicating no law, '1' indicating primary law (no other violation needed to issue ticket) and '2' indicating secondary law (There must be at least 1 other violation to issue ticket)**

**Drunk Driviing related laws, i.e. 'minage', 'zerotol', 'adm_lic_revoc_law', 'blood_alc_lim_08' and 'blood_alc_lim_10' are the laws to prevent drunk driving in a state. 'minage' reflects minimum age required to issue a driver's license. The range varies from 18 - 21 years. 'zerotol' indicates DUI offense charge for any drunk driving related traffic violation. The range varies between 0 and 1. 'adm_lic_revoc_law' enforces revocation of driver's license for drunk driver related offenses. A fraction reflects portion of the year for which the law was implemented. 'blood_alc_lim_08' and 'blood_alc_lim_10' reflect whether the allowed blood alcohol level in a driver to avoid offense is 0.08% or 0.10% respectively.**

**Speed Limit related laws, i.e. 'sl55', 'sl60', 'sl65', 'sl70', 'sl75' and 'sl70plus' are dummy variables indicating implemented speed limit in a state in a given year.**

## Data Source
**Very likely, the data is collected by Insurance Institute for Highway Safety (IIHS) or The National Highway Traffic Safety Administration (NHTSA) aided by reporting from local law enforcement. Local law enforcement, such as county police, highway patrol, or other agencies attend to most of the incidents, and report on these incidents. These are then collected, curated, and maintained by agencies such as NHTSA, IIHS for analysis and for policy formulation aimed toward reducing accidents and fatalities. The data is summarizes at yearly level for each state based on individual reports from state/county/town law enforcement, high patrol, or other similar agencies. This data doesn't appear to have been generated by a survey. It will be a non-trivial task for a survey respondent to estimate weekend fatalities, night fatalities etc., and more so the respective rates by population and/or vehicle miles driven.**

**The data appears to have been census driven across the state population, and then summarized. This data is not a sample from the population.**

**The variable "totfatrate" in the original data, which we have renamed as "total_fatality_rate" is the total number of fatal accidents per 100,000 people (all of whom may not be drivers). In the data set we also see weekend and night fatalities metrics. The rest (after subtracting night and weekend fatalities) gives the fatal accidents number during day time on weekdays.**

3. (20 points) Conduct a very thorough EDA, which should include both graphical and tabular techniques, on the dataset, including both the dependent variable `total_fatalities_rate` and the potential explanatory variables. Minimally, this should include: 
    - How is the our dependent variable of interest `total_fatalities_rate` defined? 
    - What is the average of `total_fatalities_rate` in each of the years in the time period covered in this dataset? 


```{r, fig.height=3, fig.width=8, fig.align='center', echo=FALSE}
annual_avg <- traffic_df %>%
  group_by(year) %>%
  dplyr::summarise(avg_tot_fatalities = mean(total_fatality_rate))

annual_avg %>%
  ggplot(aes(x = year, y = avg_tot_fatalities)) +
  geom_line(aes(colour = "Mean across states"), size = 1) +
  geom_smooth(method = 'lm', formula = "y~x", se = FALSE, size = 0.5, 
              aes(colour="Smoothed Mean")) +
  labs(title = "Average Total Fatality rate from 1980-2004", 
       y = "Avg total fatalities", x = "Time (year)") +
  xlim(c(1980, 2005)) +
  theme_minimal()+
  scale_colour_manual(name="Legend", values=c("red", "blue"))
```

**The plot above shows the average across states for a given year. We show both the average value as well as the smoothed value. There is definitely a downward trend in the number of accidents.**

```{r by state, fig.width=8, fig.height=8, echo=FALSE}
par(mfrow=c(8, 6))
par(mar=c(1,1,1,1))
par(mgp = c(1.5, 0.5, 0))
par(oma = c(0, 0, 3, 0))
st_code <- unique(traffic_df$state_code)
for (st in st_code) {
  traffic_state_df <- traffic_df[traffic_df$state_code == st ,]
  plot(traffic_state_df$year, traffic_state_df$total_fatality_rate, type="l",
       xlab = "Year", ylab = "Fatality per 100,000 persons", cex.axis=0.75)
  title(st, adj = 1, line = -1, cex.main=0.9)
}
mtext("Fatality Rate by State", side = 3, line = 0.25, outer = TRUE, cex=0.8)
```

>The plots above capture the traffic fatality rate (per 100,000 population) for each state. While most states show a general downward trend in the number of fatal rate there are some exceptions

* AK (Arkansas), AZ (Arizona), and MO (Missouri): These two states show upward trends before dipping down toward the very end. 

* KY (Kentucky) and MS (Mississippi): This state shows a jagged saw-tooth pattern.

* SD (South Dakota): There is an initial steep decrease followed by moderate upward trend.

```{r histograms of outcome variables, echo=FALSE, fig.width=8, fig.height=12}
par(mfrow=c(3, 3))
hist(traffic_df$total_fatalities, main="Historgam: Total Fatality", 
     xlab="Total Fatality")
hist(traffic_df$night_fatalities, main="Historgam: Night Fatality", 
     xlab="Night Fatality")
hist(traffic_df$weekend_fatalities, main="Historgam: Weekend Fatality", 
     xlab="Weekend Fatality")

hist(traffic_df$total_fatal_per_100mm, 
     main="Historgam: Total Fatality/100 Mil. Miles", cex.main=0.8,
     xlab="Total Fatality/100 Mil. miles")
hist(traffic_df$night_fatal_per_100mm, 
     main="Historgam: Night Fatality/100 Mil. Miles", cex.main=0.8,
     xlab="Night Fatality/100 Mil. miles")
hist(traffic_df$weekend_fatal_per_100mm, 
     main="Historgam: Weeknd Fatality/100 Mil. Miles", cex.main=0.8,
     xlab="Weekend Fatality/100 Mil. miles")

hist(traffic_df$total_fatality_rate, 
     main="Historgam: Total Fatality rate", xlab="Total Fatality rate")
hist(traffic_df$night_fatality_rate, 
     main="Historgam: Night Fatality rate", xlab="Night Fatality rate")
hist(traffic_df$weekend_fatality_rate, 
     main="Historgam: Weekend Fatality rate", xlab="Weekend Fatality rate")
```

```{r histograms of log of outcome variables, echo=FALSE}
par(mfrow=c(3, 3))
hist(log(traffic_df$total_fatalities), main="Historgam:Log(Total Fatality)", 
     xlab="Total Fatality")
hist(log(traffic_df$night_fatalities), main="Historgam:Log(Night Fatality)", 
     xlab="Night Fatality")
hist(log(traffic_df$weekend_fatalities), main="Historgam:Log(Weekend Fatality)", 
     xlab="Weekend Fatality")

hist(log(traffic_df$total_fatal_per_100mm), 
     main="Historgam:Log(Total Fatality/100 Mil. Miles)", cex.main=0.8,
     xlab="Total Fatality/100 Mil. miles")
hist(log(traffic_df$night_fatal_per_100mm), 
     main="Historgam:Log(Night Fatality/100 Mil. Miles)", cex.main=0.8,
     xlab="Night Fatality/100 Mil. miles")
hist(log(traffic_df$weekend_fatal_per_100mm), 
     main="Historgam:Log(Weeknd Fatality/100 Mil. Miles)", cex.main=0.8,
     xlab="Weekend Fatality/100 Mil. miles")

hist(log(traffic_df$total_fatality_rate), 
     main="Historgam:Log(Total Fatality rate)", xlab="Total Fatality rate")
hist(log(traffic_df$night_fatality_rate), 
     main="Historgam:Log(Night Fatality rate)", xlab="Night Fatality rate")
hist(log(traffic_df$weekend_fatality_rate), 
     main="Historgam:Log(Weekend Fatality rate)", xlab="Weekend Fatality rate")
```

**The two set of plots above show the histogram of the outcome variables (various fatality measure) in its native form and with log transformation. The log-transformed data shows a behavior that is more close to that of a normal distribution. As part of the modeling analysis we also conducts formal test (using Shapiro-Wilk's method). We take advantage of the log transformation in the modeling and analysis.**

**In the graphs below we show the box plots of the total fatality and total fatality rates by state and by year. Unlike the histogram, where we have shown all the dependent variables (such as nigh/weekend metrics), we focus on the total fatalities related outcomes, both by state and year.**

```{r outcome boxplots by year and state, fig.height=8, fig.width=8, echo=FALSE}
box_st_1 <- traffic_pdf %>%
  group_by(state_code) %>%
  ggplot(aes(x = reorder(state_code, total_fatalities), 
             y = total_fatalities)) +
  geom_boxplot() +
  labs(x = "State",  y = "Total Fatalities", 
       title = "Box plot of Total Fatalities by State") + 
  theme(axis.text=element_text(size=5))

box_st_2 <- traffic_pdf %>%
  group_by(state_code) %>%
  ggplot(aes(x = reorder(state_code, total_fatality_rate), 
             y = total_fatality_rate)) +
  geom_boxplot() +
  labs(x = "State",  y = "Total Fatality rate", 
       title = "Box plot of Total Fatalities rate by State") + 
  theme(axis.text=element_text(size=5))

box_st_3 <- traffic_pdf %>%
  group_by(year) %>%
  ggplot(aes(x = reorder(year, total_fatalities), 
             y = total_fatalities)) +
  geom_boxplot() +
  labs(x = "Year",  y = "Total Fatalities",
       title = "Box plot of Total Fatalities by Year")
box_st_3 + theme(axis.text=element_text(size=5))

box_st_4 <- traffic_pdf %>%
  group_by(year) %>%
  ggplot(aes(x = reorder(year, total_fatality_rate), 
             y = total_fatality_rate)) +
  geom_boxplot() +
  labs(x = "Year",  y = "Total Fatality rate",
       title = "Box plot of Total Fatality rate by Year") + 
  theme(axis.text=element_text(size=5))

grid.arrange(box_st_1, box_st_2, box_st_3, box_st_4, 
             nrow = 4, ncol = 1, 
             top=quote("Boxplot of Outcome Variables by Year"))
```

**The box plots reveal what is generally to be expected. We see a higher number with California (CA) state, for total fatality, due to the high population and generally being a somewhat of a larger state where people need to drive moderate distances, outside of major cities like Los Angeles, San Francisco, and a few others. However, if we consider fatality rate, we see that states like New Jersey (NJ), Missouri (MO), and Wyoming (WY) are leading the pack.** 

**When we look at the next two box plots, which show data across states by year, we see quite a bit of outliers. This is primarily due to the varying conditions of population, driving distances, and other demographic/economic factors. For instance, if we consider California and Wyoming, the conditions are vastly different on demographics, economics, laws such as speed limits, distance driven etc. Thus, the yearly plot tend to bring out the outliers among states. As an additional observation, we see more of these outliers when we look at the absolute fatality number, and less so with fatality rate.**

**We will now examine the histogram and box plots of the explanatory variables**

```{r explanatory variables histogram, fig.width=8, fig.height=12, echo=FALSE}
par(mfrow=c(3, 3))
hist(traffic_df$minage, main="Historgam: Minimum Age", 
     xlab="Age")
hist(traffic_df$state_population, main="Historgam: State Population", 
     xlab="Population")
hist(traffic_df$vehicle_miles_traveled_billions, main="Historgam: Vehicle Mile", 
     xlab="Vehicle Miles in Billions")
hist(traffic_df$unemployment_rate, main="Historgam: Unemployment Rate", 
     xlab="Unemployment Rate")
hist(traffic_df$percent_pop_aged_14_to_24, main="Historgam: 14 to 24 age group", 
     xlab="Population - 14 to 24 years(%)")
hist(traffic_df$miles_driven_per_capita, main="Historgam: Per capita mile", 
     xlab="Miles")
hist(traffic_df$speed_limit, main="Historgam: Speed Limit", 
     xlab="Speed Limit (mph)")
hist(traffic_df$bac08, main="Historgam: Alcohol level - 0.08%", 
     xlab="Blood Alcohol Level")
hist(traffic_df$bac10, main="Historgam: Alcohol level - 0.10%", 
     xlab="Blood Alcohol Level")

par(mfrow=c(3, 3))
hist(log(traffic_df$minage), main="Historgam: Minimum Age", 
     xlab="Log(Age)")
hist(log(traffic_df$state_population), main="Historgam: State Population", 
     xlab="Log(Population)")
hist(log(traffic_df$vehicle_miles_traveled_billions), 
     main="Historgam: Vehicle Mile", xlab="Log(Vehicle Miles in Billions)")
hist(log(traffic_df$unemployment_rate), main="Historgam: Unemployment Rate", 
     xlab="Log(Unemployment Rate)")
hist(log(traffic_df$percent_pop_aged_14_to_24), 
     main="Historgam: 14 to 24 age group", xlab="Log(14 to 24 years old (%))")
hist(log(traffic_df$miles_driven_per_capita), main="Historgam: Per capita mile", 
     xlab="Log(Miles)")
hist(log(traffic_df$speed_limit), main="Historgam: Speed Limit", 
     xlab="Log(Speed Limit (mph))")
hist(log(traffic_df$bac08), main="Historgam: Alcohol level - 0.08%", 
     xlab="Log(Blood Alcohol Level)")
hist(log(traffic_df$bac10), main="Historgam: Alcohol level - 0.10%", 
     xlab="Log(Blood Alcohol Level)")
```

**From the histograms above we see that some of the left-skewed histogram of the variables in its natural form is coming closer to that of a normal distribution, when log-transformed. Notable exceptions to the statement just made are the discrete variables, in particular, Minimum Age and Speed Limit variables. The reasons are obvious. Log transformation doesn't change the fundamentally non-continuous nature of these variables. As alluded to earlier, we will take advantage of the log-transform in the models.**

```{r ind var by state, warning=FALSE, echo=FALSE, fig.height=10, fig.width=8}
box_ex_1 <- traffic_pdf %>% group_by(state_code) %>%
  ggplot(aes(x = reorder(state_code, state_population), y = state_population)) +
  geom_boxplot() + labs(x = "States",  y = "Population") +
  theme(axis.text=element_text(size=5))

box_ex_2 <- traffic_pdf %>% group_by(state_code) %>%
  ggplot(aes(x = reorder(state_code, vehicle_miles_traveled_billions), 
             y = vehicle_miles_traveled_billions)) +
  geom_boxplot() + labs(x = "States",  y = "Miles Travelled") +
  theme(axis.text=element_text(size=5))

box_ex_3 <- traffic_pdf %>% group_by(state_code) %>%
  ggplot(aes(x = reorder(state_code, unemployment_rate), 
             y = unemployment_rate)) +
  geom_boxplot() + labs(x = "States",  y = "Unemp. rate") +
  theme(axis.text=element_text(size=5))

box_ex_4 <- traffic_pdf %>% group_by(state_code) %>%
  ggplot(aes(x = reorder(state_code, percent_pop_aged_14_to_24), 
             y = percent_pop_aged_14_to_24)) +
  geom_boxplot() + labs(x = "States",  y = "% Pop. 14-24") +
  theme(axis.text=element_text(size=5))

box_ex_5 <- traffic_pdf %>% group_by(state_code) %>%
  ggplot(aes(x = reorder(state_code, miles_driven_per_capita), 
             y = miles_driven_per_capita)) +
  geom_boxplot() + labs(x = "States",  y = "Miles per Capita") +
  theme(axis.text=element_text(size=5))

box_ex_6 <- traffic_pdf %>% group_by(state_code) %>%
  ggplot(aes(x = reorder(state_code, speed_limit), y = speed_limit)) +
  geom_boxplot() + labs(x = "States",  y = "Speed limit") +
  theme(axis.text=element_text(size=5))

grid.arrange(box_ex_1, box_ex_2, box_ex_3, box_ex_4, box_ex_5, box_ex_6, 
             nrow = 6, ncol = 1, 
             top=quote("Boxplot of Explanatory Variables by State"))
```

**We see from the box plots above, of the explanatory variables by state, CA has the highest population, mean unemployment rate is  very close among the states with a few outliers, and the miles traveled is led by CA, being a populous and a large state. However, the miles per-capita is being led by Wyoming. This is not entirely unexpected. A state as vast a Wyoming and with a smaller population people do drive a lot in Wyoming to get around. It is also a rural state where people don't find a lot of what they want nearby. Additionally, it's not uncommon to see animals and produce transported in smaller trucks from farms to markets. As for speed limits, it appears that that are just two average speed limits across states - 55mph and 65mph. We do see lower speed limits in Montana (MT). We also see higher speed limits of 70mph is some states. As highway quality and vehicle features improve we see higher speed limits.**

```{r ind variables boxplots by year, echo=FALSE, fig.height=10, fig.width=8}
box_ex_1 <- traffic_pdf %>% group_by(year) %>%
  ggplot(aes(x = year, y = state_population)) +
  geom_boxplot() + labs(x = "Year",  y = "Population") +
  theme(axis.text=element_text(size=5))

box_ex_2 <- traffic_pdf %>% group_by(year) %>%
  ggplot(aes(x = year, y = vehicle_miles_traveled_billions)) +
  geom_boxplot() + labs(x = "Year",  y = "Miles Travelled") +
  theme(axis.text=element_text(size=5))

box_ex_3 <- traffic_pdf %>% group_by(year) %>%
  ggplot(aes(x = year, y = unemployment_rate)) +
  geom_boxplot() + labs(x = "Year",  y = "Unemp. rate") +
  theme(axis.text=element_text(size=5))

box_ex_4 <- traffic_pdf %>% group_by(year) %>%
  ggplot(aes(x = year, y = percent_pop_aged_14_to_24)) +
  geom_boxplot() + labs(x = "Year",  y = "% Pop. 14-24") +
  theme(axis.text=element_text(size=5))

box_ex_5 <- traffic_pdf %>% group_by(year) %>%
  ggplot(aes(x = year, y = miles_driven_per_capita)) +
  geom_boxplot() + labs(x = "Year",  y = "Miles per Capita") +
  theme(axis.text=element_text(size=5))

box_ex_6 <- traffic_pdf %>% group_by(year) %>%
  ggplot(aes(x = year, y = speed_limit)) +
  geom_boxplot() + labs(x = "Year",  y = "Speed limit") +
  theme(axis.text=element_text(size=5))

grid.arrange(box_ex_1, box_ex_2, box_ex_3, box_ex_4, box_ex_5, box_ex_6, 
             nrow = 6, ncol = 1, 
             top=quote("Boxplot of Explanatory Variables by Year"))
```

**The box plots above are across states for each year. The population averages across states has remained more or less same across years. We see a lot of outliers due to movement of people. Relocation happens every so often for various reasons, and there may be an instantaneous increase or decrease of population in some states in some of the years. The outliers reflect this reality. The miles traveled per-capita shows a steady increase albeit by a small amount. The speed limit has gone up steadily. As we make better roads and vehicles, and improve fuel efficiency, people tend to drive more. Areas that didn't have roads have opened up leading to residential buildings in places that were once off-limit. This has caused more driving. The average unemployment rate has remined nearly constant barring blips in 1981-82, 1991-92, and 2001-2002; the classic 10-year cycle. The population in 14-24 age group decresed initially, but seems to be going up although very moderately. The outliers capture the uneven distribution across states.** 

```{r x-corr with econ/demograhics, warning=FALSE, message=FALSE, error=FALSE, fig.height=8, fig.width=8}
# annual mean
annual_mean <- aggregate(
  traffic_df[, c("total_fatality_rate", "state_population",
                 "unemployment_rate","percent_pop_aged_14_to_24",
                 "vehicle_miles_traveled_billions", "miles_driven_per_capita")], 
  traffic_df["year"], FUN = mean)

ggpairs(annual_mean, size = 5, cardinality_threshold = 25) + 
  theme(strip.text.x = element_text(size = 5),
        strip.text.y = element_text(size = 5)) +
  theme(axis.text = element_text(size = 5)) +
  ggtitle(label = "State Averages over time (years)")

# state mean
state_mean <- aggregate(
  traffic_df[, c("total_fatality_rate", "state_population",
                 "unemployment_rate", "percent_pop_aged_14_to_24",
                 "vehicle_miles_traveled_billions","miles_driven_per_capita")], 
  traffic_df["state_code"], FUN = mean)

s <- ggpairs(state_mean, size = 5, cardinality_threshold = 60) + 
       theme(strip.text.x = element_text(size = 5),
             strip.text.y = element_text(size = 5)) +
       theme(axis.text = element_text(size = 5)) +
       ggtitle(label = "Year Averages by State")

s$plots <- s$plots[-(seq(1, s$ncol*s$nrow, by = s$ncol))]
s$ncol <- s$ncol - 1
s$xAxisLabels <- s$xAxisLabels[-1]
s
```

**The first plot above show the cross-correlations of among annual mean of total fatality rate against economic, demographic, and vehicle miles.**

**Observations:**
1. *state_population* increase with time which makes sense
2. *vehicle_miles_traveled_billions* increase with time as more people drive more miles over years
3. *miles_driven_per_capita* increase with time probably because cars are more affordable and hence accessible
4. *percent_pop_aged_14_to_24* decreases indicating aged and experienced population resulting into fewer fatalities
5. *unemployment_rate* is cyclical but decreasing overall over years
6. *total_fatality_rate* decreases with time as we observed earlier

We observe covariance between pairs of variables as well but that probably is more of a spurious correlation.

**We also show the mean for each state of fatality rate and its cross-correlation with the same set of variables.**
**Observations:**
1. *total_fatality_rate* shows high positive correlation with *miles_driven_per_capita* meaning more time spent on road means increased probability of fatality. 
2. *total_fatality_rate* shows moderate positive correlation with *percent_pop_aged_14_to_24* meaning higher youth proportion, higher their fatality rate. 
3. *percent_pop_aged_14_to_24* shows moderate positive correlation with *miles_driven_per_capita* meaning states with higher young generation drives more, resulting into higher fatality rate, which could be due to negligence driving habits of younger drivers overall. 
4. *state_population* shows strong positive correlation with *vehicle_miles_traveled_billions* meaning more people drive more. 
5. *state_population* shows moderate negative correlation with *total_fatality_rate* meaning more people more enforcing safe driving infrastructure. 
6. *state_population* shows moderate negative correlation with *miles_driven_per_capita* meaning states with larger population drive lesser, leading to lower on road fatalities 

**We now show similar cross-correlation for the total fatality rate with, both annually and by state, on the laws of the states**

```{r x-corr with laws, warning=FALSE, message=FALSE, error=FALSE, fig.height=10, fig.width=8}
annual_law_mean <- aggregate(
  traffic_df[,c("total_fatality_rate", "minage", "zerotol", "sl70plus",
                "grad_drivers_lic_law", "adm_lic_revoc_law", "blood_alc_lim_10",
                "blood_alc_lim_08", "primary_seatbelt_law",
                "secondary_seatbelt_law")], 
  traffic_df["year"], FUN = mean)

ggpairs(annual_law_mean, size = 5, cardinality_threshold=100,
        upper = list(continuous = wrap("cor", size = 3))) + 
  theme(strip.text.x = element_text(size = 5),
        strip.text.y = element_text(size = 5)) +
  theme(axis.text = element_text(size = 5)) +
  ggtitle(label = "State Law Proportions over time (years)")

state_law_mean <- aggregate(
  traffic_df[,c("total_fatality_rate", "minage", "zerotol", "sl70plus", 
                "grad_drivers_lic_law", "adm_lic_revoc_law", "blood_alc_lim_10",
                "blood_alc_lim_08", "primary_seatbelt_law",
                "secondary_seatbelt_law")], 
  traffic_df["state"], FUN = mean)
l <- ggpairs(state_law_mean, size = 5, cardinality_threshold=48,
             upper = list(continuous = wrap("cor", size = 3))) + 
       theme(strip.text.x = element_text(size = 5),
             strip.text.y = element_text(size = 5)) +
      theme(axis.text = element_text(size = 5)) + 
      ggtitle(label = "State Law Time Proportions vs State")

l$plots <- l$plots[-(seq(1, l$ncol*l$nrow, by = l$ncol))]
l$ncol <- l$ncol - 1
l$xAxisLabels <- l$xAxisLabels[-1]
l
```

**The first plot above show the cross-correlations of proportions of time (years) states implemented various traffic law verses, i.e. how the adaptability of these laws have changed over time.**

**Observations:**
1. *grad_drivers_lic_law* shows strong positive correlation with *primary_seatbelt_law* meaning if in any given year, a large number of states has implemented one law, then large number of states will implement the other law as well 
2. *blood_alc_lim_08* shows high positive correlation over time indicating that larger and larger number of states have implemented that law with time. 
3. *blood_alc_lim_10* shows low negative correlation over time indicating that lesser number of states have implemented that law with time. It is expected given that *blood_alc_lim_08* is more popular among states 
4. Other laws *minage, zerotol, sl70plus, grad_drivers_lic_law, adm_lic_revoc_law* also show string positive correlation over time indicating that larger and larger number of states have implemented that law with time. 


**The second plot above show the cross-correlations of proportions of time (years) states implemented various traffic law verses states**

**Observations:**
1. *primary_seatbelt_law* shows string negative correlation with *secondary_seatbelt_law* meaning if a state has implemented one law, they dont implement other and it makes sense as these laws appear mutually exclusive 
2. *blood_alc_lim_08* shows high negtive correlation with *blood_alc_lim_10* meaning state would implement one of the two. In other words, if a state implements one law for larger number of years, it would not implements the other. 
3. *zerotol* shows moderate positive correlation with *blood_alc_lim_08* meaning longer one state one of the law implemented, longer we expect that state to implement the other one two. 

# (15 points) Preliminary Model

Estimate a linear regression model of *totfatrte* on a set of dummy variables for the years 1981 through 2004 and interpret what you observe. In this section, you should address the following tasks: 

- Why is fitting a linear model a sensible starting place? 
- What does this model explain, and what do you find in this model? 
- Did driving become safer over this period? Please provide a detailed explanation.
- What, if any, are the limitation of this model. In answering this, please consider **at least**: 
    - Are the parameter estimates reliable, unbiased estimates of the truth? Or, are they biased due to the way that the data is structured?
    - Are the uncertainty estimate reliable, unbiased estimates of sampling based variability? Or, are they biased due to the way that the data is structured? 
    
```{r base model}
base_model <- lm(total_fatality_rate ~ d81 + d82 + d83 + d84 + d85 + d86 + d87 + 
                   d88 + d89 + d90 + d91 + d92 + d93 + d94 + d95 + d96 + d97 +
                   d98 + d99 + d00 + d01 + d02 + d03 + d04, data = traffic_df) 
summary(base_model)
```

```{r examine the model}
aug_base_model <- augment(base_model)

aug_base_model %>%
  ggplot(aes(x=1:1200)) +
  geom_line(aes(y = total_fatality_rate, colour = "Total Fatality rate"), 
            size = 1) +
  geom_line(aes(y = .fitted, colour = "Fitted Values"), size = 1) + 
  geom_line(aes(y = .resid, colour = "Residual Values"), size = 1) +
  labs(title = "Actual Vs Fitted value", x = "Index", 
       y = "Actual/Fitted/Residuals") +
  scale_colour_manual(name="Legend", values=c("red", "blue", "green"))

shapiro.test(aug_base_model$.resid) # test for normal distribution

t.test(augment(base_model)$.fitted, 
       augment(lm(total_fatality_rate ~ 1, data=traffic_df))$.fitted)
```

### Why fit a linear model as a starting point: 
We have 1200 observations, which is a reasonably large number of observations. Granted that these observations aren't completely independent of each other. For a given state the year-over-year may bear some correlation. That said, the time frame covers 25years, and this can give enough "spacing" between data points that there are uncorrelated observations. The data across states can be considered independent. States have high degree of autonomy to set their own rules. Usually, it is the case that a linear model does as good a job as others if we have sufficient amount of data. 

### Model explanation: 
This model basically checks to see if we can fit all the existing data as a linear combination of year dummy variables. In others words, can some linear combination of years able to do a reasonable fit given that such that the sum of difference of squares between predicted and actual values are minimized? A few additional points:

A regression with only Dummy Variables is the ANOVA model. The intercept gives the mean value of traffic fatality rate. The coefficients associated with the dummy variables are not really the "slope" (due to non-continuous variable value); rather it's the differential intercept coefficient, which tells by how the intercept differs going from year to year.

The model output gives most weight to the intercept and it is statistically significant. This shows that irrespective of the year variable there is a base fatality rate.

### Did driving become safer over this period?
As we move from 1980 to 2004 we see a reduction in fatality. See the plot below. While the reduction is not monotonic with year the overall trend is toward reduction. Barring year 1981 all other year coefficients are statistically significant. The highest reduction is in 2004 (`r as.numeric(coef(base_model)[25])`).  We do see an increase from 1985 to 1989. However, for the most part the plot shows that as we move from 1980 toward 2004 we see a reduction in the fatality rate. This could be explained through a combination of technology (better cars, better roads, safety improvements) and, potentially, additional laws. In summary, we see that 2004 has fatality rate reduced by `r as.numeric(coef(base_model)[25])` when compared to the base year of 1980.

```{r fatality coeff, echo=FALSE}
plot(1981:2004, coef(base_model)[2:25], type="l", xlab="Years", 
     ylab="Coefficients", main="Regression coefficinets Vs Years" )
```

### Model limitations
The residuals have low mean values (`r mean(aug_base_model$.resid, na.rm = TRUE)`). However the Shapiro-Wilk's test doesn't give statistical confidence to assume that the residuals are normally distributed. The R-squared value of the base model is quite low indicating that the model didn't do a good job of capturing all the variance.

The plot of the actual Vs the fitted value shows that we don't have a real good prediction. Above, we also show the result of t-test between the fitted value with dummy variables, and the one without any dummy variable. The result of the t-test shows that there is no difference in the mean.

In mathematical terms, the presence of only dummy regression gives the X matrix (whose columns are the 25 dummy variables with 1200 entries each) full rank ($X^TX$ is invertible). Additionally, the matrix X is orthogonal (i.e the dot product of any two columns is zero). The norm of each column vector is $\sqrt48 = 4\sqrt3$. If we divide matrix X by $4\sqrt3$ we get an orthonormal matrix (i.e $X^TX = I$). Thus, the coefficients, $\hat\beta$, is given by $\hat\beta = (X^TX)^{-1}X^TY$ is indeed $X^TY$. This means that we can recover the predicted values without any error!! However, this is not what we get in reality. The matrix X is sometimes not even full rank, let alone being orthogonal or orthonormal. Clearly, the model assumption is not rooted in reality.

**In summary, the linear model with dummy variables is not a reliable model. Common sense also guides us that the year number alone cannot predict the traffic fatality rate. Given the pure discrete nature of data we can't expect this model to produce unbiased estimate. The same logic extends to the uncertainty estimate. We can't expect this model to recover the residuals correctly. The mathematics behind this, explained above, adds additional reasons why this model is not realistic.** 

# (15 points) Expanded Model 

Expand the **Preliminary Model** by adding variables related to the following concepts: 

- Blood alcohol levels 
- Per se laws
- Primary seat belt laws (Note that if a law was enacted sometime within a year the fraction of the year is recorded in place of the zero-one indicator.)
- Secondary seat belt laws 
- Speed limits faster than 70 
- Graduated drivers licenses 
- Percent of the population between 14 and 24 years old
- Unemployment rate
- Vehicle miles driven per capita. 

If it is appropriate, include transformations of these variables. Please carefully explain carefully your rationale, which should be based on your EDA, behind any transformation you made. If no transformation is made, explain why transformation is not needed. 

- How are the blood alcohol variables defined? Interpret the coefficients that you estimate for this concept. 
- Do *per se laws* have a negative effect on the fatality rate? 
- Does having a primary seat belt law? 

```{r expanded model}
expanded_model <- lm(total_fatality_rate ~ d81 + d82 + d83 + d84 + d85 + d86 + 
                       d87 + d88 + d89 + d90 + d91 + d92 + d93 + d94 + d95 + 
                       d96 + d97 + d98 + d99 + d00 + d01 + d02 + d03 + d04 + 
                       blood_alc_lim_08 + blood_alc_lim_10 + adm_lic_revoc_law + 
                       primary_seatbelt_law + secondary_seatbelt_law+ sl75 + 
                       slnone + grad_drivers_lic_law + 
                       percent_pop_aged_14_to_24 + log(unemployment_rate) + 
                       log(miles_driven_per_capita), data = traffic_df) 
summary(expanded_model)
```

### Transformation:
It is not uncommon to see data that are skewed. For instance, certain age group may show higher participation, which then leads to data being skewed to this age group when we look at the data across all age groups. We also see huge variance in data. In these scenarios, log transformation brings a level of normality to the data distribution. As long as the data is positive log transformation is a way of flattening the data, while preserving monotonicity. For instance, $x^2$ (convex) when log-transformed becomes $2log(x)$ (concave). 

The earlier EDA showed the histograms of the data and its log-transformed version. We show below the result of Shapiro-Wilk's test. We chose to transform only the unemployment rate (where log transform makes it normal) and miles driven (where log transform shows considerable improvement). The population percentage doesn't show as much improvement with log transform.

```{r shapiro test}
var_name <- c()
p_value <- c()

tres <- shapiro.test(log(traffic_df$percent_pop_aged_14_to_24))
var_name <- c(var_name, tres$data.name)
p_value <- c(p_value, tres$p.value)

tres <- shapiro.test(traffic_df$percent_pop_aged_14_to_24)
var_name <- c(var_name, tres$data.name)
p_value <- c(p_value, tres$p.value)

tres <- shapiro.test(log(traffic_df$unemployment_rate))
var_name <- c(var_name, tres$data.name)
p_value <- c(p_value, tres$p.value)

tres <- shapiro.test(traffic_df$unemployment_rate)
var_name <- c(var_name, tres$data.name)
p_value <- c(p_value, tres$p.value)

tres <- shapiro.test(log(traffic_df$miles_driven_per_capita))
var_name <- c(var_name, tres$data.name)
p_value <- c(p_value, tres$p.value)

tres <- shapiro.test(traffic_df$miles_driven_per_capita)
var_name <- c(var_name, tres$data.name)
p_value <- c(p_value, tres$p.value)

data.frame(variable = var_name, p_value)
```

### BAC definition and interpretation
The variables "bac08" and "bac10" indicate whether the state had a BAC limit of 0.08% and 0.10% respectively during that year. We transformed "bac08" and "bac10" variables into its binary form represented by "blood_alc_lim_10" and "blood_alc_lim_08". We use the binary form in the regression. 

The coefficients of "blood_alc_lim_08" -1.59190 with Std. Error of 0.45346, and that for "blood_alc_lim_10" is -0.62036  with Std. Error of 0.33589. While bac10 reduced the fatality rate it's not significant at 95% confidence level. The bac08 did reduce the fatality and it is statistically significant. It is much further away than two standard deviation. Thus, lower tolerance for blood alcohol content does reduce the law, and the level of 0.08% seems to do a good job of reducing fatalities in a significant way. Common sense thinking supports this result. 

### Effect of per se law
The per se law codified by the variable "grad_drivers_lic_law" has a coefficient of -0.33880 with Std. Error of 0.52333, and thus not statistically significant. The associated p-value also reveals this. It has a small statistically insignificant negative rate.

### Primary seat belt law
This is represented by the variable "primary_seatbelt_law" with coefficient 0.14036 and Std. Error 0.48367 does not have a statistically significant effect (even at 0.1 level). The coefficient is small, indicating that the effect, if at all, is small.

# (15 points) State-Level Fixed Effects 

Re-estimate the **Expanded Model** using fixed effects at the state level. 

- What do you estimate for coefficients on the blood alcohol variables? How do the coefficients on the blood alcohol variables change, if at all? 
- What do you estimate for coefficients on per se laws? How do the coefficients on per se laws change, if at all? 
- What do you estimate for coefficients on primary seat-belt laws? How do the coefficients on primary seatbelt laws change, if at all? 

Which set of estimates do you think is more reliable? Why do you think this? 

- What assumptions are needed in each of these models?  
- Are these assumptions reasonable in the current context?

```{r Fixed Effects models}
pool.model <- plm(total_fatality_rate ~ year + bac08 + bac10 + 
                    adm_lic_revoc_law + primary_seatbelt_law + 
                    secondary_seatbelt_law + sl75 + slnone + 
                    grad_drivers_lic_law + percent_pop_aged_14_to_24 + 
                    log(unemployment_rate) + 
                    log(miles_driven_per_capita), data=traffic_df, 
                  index=c("state", "year"), model="pooling")
fd.model <- plm(total_fatality_rate ~ year + bac08 + bac10 + 
                  adm_lic_revoc_law + primary_seatbelt_law + 
                  secondary_seatbelt_law + sl75 + slnone + 
                  grad_drivers_lic_law + percent_pop_aged_14_to_24 +
                  log(unemployment_rate) + log(miles_driven_per_capita), 
                data=traffic_df, index=c("state", "year"), model="fd")
between.model <- plm(total_fatality_rate ~ year + bac08 + bac10 + 
                       adm_lic_revoc_law + primary_seatbelt_law + 
                       secondary_seatbelt_law + sl75 + slnone + 
                       grad_drivers_lic_law + percent_pop_aged_14_to_24 + 
                       log(unemployment_rate) + log(miles_driven_per_capita), 
                     data=traffic_df, index=c("state", "year"), 
                     model="between")
within.model <- plm(total_fatality_rate ~ year + bac08 + bac10 + 
                      adm_lic_revoc_law + primary_seatbelt_law + 
                      secondary_seatbelt_law + sl75 + slnone + 
                      grad_drivers_lic_law + percent_pop_aged_14_to_24 + 
                      log(unemployment_rate) + log(miles_driven_per_capita), 
                    data=traffic_df, index=c("state", "year"), 
                    model="within")
```

```{r model ouputs, echo=FALSE}
stargazer(pool.model, fd.model, between.model, within.model, type = "text",
          omit.stat = c("ser","f","adj.rsq"), dep.var.labels = "",
          column.labels = c("Pooled", "FD", "Between", "Within"))
```

## Blood Alcohol variable coefficient exploration
The variables "bac08" and "bac10" explains the effect of the states implementing the blood alcohol level laws. The Pooled model shows that for states that implemented "bac08" saw a reduction of 2.431 in the fatality rate, and it is statistically significant. The impact of "bac10" on fatality rate is relatively lower at 1.257, and is statistically significant. The "within" model reflects similar results with a bac08/10 coefficient that reduces fatality by 1.222 and 0.912, respectively. The coefficients are lower because we subtract the mean values of the variables in the "within" model. As for First Difference (FD) model, the variable is zero if a state had implemented the law in consecutive years. Thus, the FD tends to minimize the effect of these variables. We get similar results for the coefficients. The reason for bac08 getting higher impact compared to bac10 is due to the number of observations relatively more skewed to 1.0. Finally, for the "between" model we take average for a unit across all time periods, thus the effects are pronounced higher. We expect the model to not show significance likely due to the different times when each state implemented the law. As expected, the standard error is higher for "between" model when compared to other models.

## Per se laws variable coefficient exploration
The variable "adm_lic_revoc_law" (alias "perse") explains the effect of the states implementing the perse law - "suspend or revoke the driving privilege of persons who are arrested for driving with a BAC of . 08% or more". The Pooled model shows that for states that implemented "perse" saw a reduction of 0.461 in the fatality rate, and it is not statistically significant. The "FD" model reflects similar results, with a reduction of 0.613 and the "within" model illustrates significant result with a reduction of 1.107 in fatality rate. As for First Difference (FD) model, the variable is zero if a state had implemented the law in consecutive years. Thus, the FD tends to minimize the effect of these variables. This leads to a coefficient value that is not statistically significant. Finally, for the "between" model we take average for a unit across all time periods, thus the coefficient shows a positive number with no significance. We expect the model to not show significance likely due to the different times when each state implemented the law. The averaging effect diminished the values of this variable. As expected, the standard error is higher for "between" model when compared to other models.

## Primary seat-belt variable coefficient exploration
The variable "primary_seatbelt_law" explains the effect of the states mandating "driver to wear the seat belt" as a law. The Pooled model shows that for states that implemented "primary_seatbelt_law" saw an increase in fatality of 0.192, but it is not statistically significant. In order for pooled OLS to produce a consistent estimator of $\beta_1$, we would have to assume that the unobserved effect, $a_i$, is uncorrelated with $x_{it}$. Assuming that correlation between $a_i$ and $x_{it}$ exists we end up with heterogeneity bias or bias caused from omitting a time-constant variable. The "FD" model reflects a reduction of 0.355 and the "within" model illustrates significant result with a reduction of 1.159 in fatality rate. As for First Difference (FD) model, the variable is zero if a state had implemented the law in consecutive years. Thus, the FD tends to minimize the effect of these variables. This leads to a coefficient value that is not statistically significant. Finally, for the "between" model we take average for a unit across all time periods thus the coefficient shows larger number with no significance. As expected the standard error is very high for "between" when compared to other models.

## Reliable model and the reason behind it

First lets explore the assumptions 

A1- Linearity: the model is linear in parameters

A2- i.i.d. : The observations are independent across individuals but not necessarily across time. This is guaranteed by random sampling of individuals.

A3- Identifiability: The regressors, including a constant, are not perfectly collinear, and all regressors (but the constant) have non-zero variance and not too many extreme values.

A4- $x_it$ is uncorrelated with idiosyncratic error term $u_{it}$ and individual-specific effect $\gamma_i$ 
  + $E(u_{it}x_{it})$ = 0
  + $E(x_{it}, \gamma_i)$ = 0

A5- Zero conditional (strict exogeneity) - The most important of these is that $\delta{u_i}$ is uncorrelated with $\delta{x_i}$. This assumption holds if the idiosyncratic error at each time t, $u_{it}$, is uncorrelated with the explanatory variable in both time periods. 

For each models the assumptions that should hold true are

**Pooled OLS Model Assumptions** - A1,A2,A3,A4
**First Difference Estimator Assumptions** - A1,A2,A3,A5
**Within Model Assumptions** - A1,A2,A3,A5
**Between Model Assumptions** - A1,A2,A3,A5

**A1** - Linearity:

```{r variable lienarity, echo=FALSE, warning=FALSE}
 traffic_df %>%
  ggplot(aes(bac08,total_fatality_rate,color=state)) +
  facet_wrap(~state) +
  geom_point(alpha=0.4) +
  geom_smooth(method="lm",color="black") +
  theme_economist_white(gray_bg=F) +
  theme(legend.position="none") +
  xlab("bac08") +
  ylab("Fatality")

 traffic_df %>%
  ggplot(aes(bac10,total_fatality_rate,color=state)) +
  facet_wrap(~state) +
  geom_point(alpha=0.4) +
  geom_smooth(method="lm",color="black") +
  theme_economist_white(gray_bg=F) +
  theme(legend.position="none") +
  xlab("bac10") +
  ylab("Fatality")

  traffic_df %>%
  ggplot(aes(adm_lic_revoc_law,total_fatality_rate,color=state)) +
  facet_wrap(~state) +
  geom_point(alpha=0.4) +
  geom_smooth(method="lm",color="black") +
    theme_economist_white(gray_bg=F) +
  theme(legend.position="none") +
  xlab("perse") +
  ylab("Fatality")

 traffic_df %>%
  ggplot(aes(primary_seatbelt_law,total_fatality_rate,color=state)) +
  facet_wrap(~state) +
  geom_point(alpha=0.4) +
  geom_smooth(method="lm",color="black") +
   theme_economist_white(gray_bg=F) +
  theme(legend.position="none") +
  xlab("prim seat belt") +
  ylab("Fatality")

traffic_df %>%
  ggplot(aes(secondary_seatbelt_law,total_fatality_rate,color=state)) +
  facet_wrap(~state) +
  geom_point(alpha=0.4) +
  geom_smooth(method="lm",color="black") +
  theme_economist_white(gray_bg=F) +
  theme(legend.position="none") +
  xlab("secon seat belt") +
  ylab("Fatality")

traffic_df %>%
  ggplot(aes(sl75,total_fatality_rate,color=state)) +
  facet_wrap(~state) +
  geom_point(alpha=0.4) +
  geom_smooth(method="lm",color="black") +
  theme_economist_white(gray_bg=F) +
  theme(legend.position="none") +
  xlab("speed 75") +
  ylab("Fatality")

traffic_df %>%
  ggplot(aes(slnone,total_fatality_rate,color=state)) +
  facet_wrap(~state) +
  geom_point(alpha=0.4) +
  geom_smooth(method="lm",color="black") +
  theme_economist_white(gray_bg=F) +
  theme(legend.position="none") +
  xlab("speed none") +
  ylab("Fatality")
traffic_df %>%
  ggplot(aes(grad_drivers_lic_law,total_fatality_rate,color=state)) +
  facet_wrap(~state) +
  geom_point(alpha=0.4) +
  geom_smooth(method="lm",color="black") +
  theme_economist_white(gray_bg=F) +
  theme(legend.position="none") +
  xlab("license") +
  ylab("Fatality")

traffic_df %>%
  ggplot(aes(percent_pop_aged_14_to_24,total_fatality_rate,color=state)) +
  facet_wrap(~state) +
  geom_point(alpha=0.4) +
  geom_smooth(method="lm",color="black") +
  theme_economist_white(gray_bg=F) +
  theme(legend.position="none") +
  xlab("age 14- 24") +
  ylab("Fatality")

traffic_df %>%
  ggplot(aes(log(unemployment_rate),total_fatality_rate,color=state)) +
  facet_wrap(~state) +
  geom_point(alpha=0.4) +
  geom_smooth(method="lm",color="black") +
  theme_economist_white(gray_bg=F) +
  theme(legend.position="none") +
  xlab("log unemployment") +
  ylab("Fatality")

traffic_df %>%
  ggplot(aes(log(miles_driven_per_capita),total_fatality_rate,color=state)) +
  facet_wrap(~state) +
  geom_point(alpha=0.4) +
  geom_smooth(method="lm",color="black") +
  theme_economist_white(gray_bg=F) +
  theme(legend.position="none") +
  xlab("log miles_driven_per_capita") +
  ylab("Fatality")
```
The above plot for each independent **variable** used in the model illustrates the relationship between dependent and the independent variables is linear.

**A2** - i.i.d: In the case of all models, except the "between" model we have 1200 data points where a single unit (state in this case) is measured multiple times (across 25 years). Clearly, measurement of a unit repeatedly is not independent. Thus, the 1200 observations are not IID.  That said, with 1200 data points we could argue that there are enough observations that are independent. Observations across states qualify for being independent. Observations within an unit across 25 years may also be independent across some of the years.

**A3**- Identifiability:
Proof: The regressors are not perfectly collinear, including a constant

Variance inflation factor (VIF) is a measure of the amount of multicollinearity in a set of multiple regression variables. A high VIF indicates that the associated independent variable is highly collinear with the other variables in the model. The below result illustrates low VIF (all below 8) the regressors are not perfectly collinear. When we see multiple variables with high VIF score (conventionally, above 8) then the test recommends that we retain one, and drop the others. The VIF scores tell us that these variables are linearly related, and hence can be dropped if we retain only one.

Multicollinearity should not be confused with a raw strong correlation between predictors. What matters is the association between one or more predictor variables, conditional on the other variables in the model. In a nutshell, multicollinearity means that once you know the effect of one predictor (bac08), the value of knowing the other predictor (bac10) is rather low. Thus, one of the predictors doesn't help much in terms of better understanding the model or predicting the outcome given that the other one is part of the model. 

```{r collinearity test}
check_collinearity(pool.model)
```
Proof: The regressors have non-zero variance and not too many extreme values (but constant). Based on the below variance information, the independent variables have non-zero variance and not too many extreme values.

```{r non-zero variance proof}
var(traffic_df$bac08)
var(traffic_df$bac10)
var(traffic_df$adm_lic_revoc_law)
var(traffic_df$sl75)
var(traffic_df$slnone)
var(traffic_df$grad_drivers_lic_law)
var(traffic_df$percent_pop_aged_14_to_24)
var(log(traffic_df$unemployment_rate))
var(log(traffic_df$miles_driven_per_capita))
```

**A4**- $x_it$ is uncorrelated with idiosyncratic error term $u_{it}$ and individual-specific effect $\gamma_i$ 

Durbin-Watson and Breusch-Godfrey/Wooldridge test (with order 2) are used to validate this.
  
```{r A4 test}
pdwtest(pool.model)
pbgtest(pool.model, order = 2)

pdwtest(fd.model)
pbgtest(fd.model, order = 2)

pdwtest(within.model)
pbgtest(within.model, order = 2)

pdwtest(between.model)
pbgtest(between.model, order = 2)
```

**We don’t have evidence to reject the null hypothesis of no serial correlation in the between models. We do have evidence to reject the null hypothesis of no serial correlation in the within and pooled OLS models. For first difference estimator model, Durbin-Watson and Breusch-Godfrey/Wooldridge test shows contradictory results. The first difference model usually ends up with correlated error terms, except when the error is random walk. For a generic AR(1) process we do expect to see serial correlation.**

**A5**- Zero conditional (strict exogeneity) - The most important of these is that $\delta{u_i}$ is uncorrelated with $\delta{x_i}$.

```{r check for exogenity pool}
df <- traffic_df
df$`log(unemployment_rate)` <- log(df$unemployment_rate)
df$`log(miles_driven_per_capita)` <- log(df$miles_driven_per_capita)
df$fit_model <- augment(pool.model)$.fitted
df$resid_model <- residuals(pool.model)

df_model <- data.frame(matrix(ncol = 3, nrow = 0))
x <- c("year", "variable", "covariance")
colnames(df_model) <- x

for (t in unique(df$year)){
  for (var_name in colnames(pool.model$model)[c(-1)]){
    for (s in unique(df$year)){
      c = cov(df[df$year == t,][var_name], df[df$year == s,]$resid_model)
      df_model <- rbind(df_model,data.frame(year = t, variable = var_name, 
                                            covariance = c[1]))
    }
  }
}
```
```{r exogenity plot pooled, echo=FALSE}
ggplot(df_model, aes(x = year, y = covariance, col = variable)) + geom_point() +
    xlab("Fitted values") + ylab("Covariance") + 
  ggtitle("Strict Exogenity of Pooled Model")
```

**Significant number of correlations are non-zero, which means that the assumption of lack of correlation is incorrect. Hence the estimates are not unbiased; the model may be consistent, though. EDA showed that there was an overall increasing trend in miles driven and an overall decreasing trend in total fatality rate. The plot above shows that there is a violation in the strict exogeneity assumption. However we chose to include the per-capita miles driven variable as miles driven has to be an explanatory variable in predicting total fatality rate (if miles driven is zero the total fatalities from auto accidents would be zero too).**

## Verdict
Pooled model uses all the observations as if these are independently measured. In general, for panel data, we know that this assumption is not true. If assumptions $A1 - A4$ are valid then pooled OLS will give consistent results. For the data set we have these assumptions don't hold good - in particular the IID assumption (A2) is not valid. Thus, pooled estimates are not unbiased and likely not consistent either. We must recognize that the pooled model had $R^2 = 0.628$, which means that the explanatory variables do capture most of the variance. This shouldn't be a surprise given that we have 1200 observations. As the number of observations go up the pooled OLS comes close to "within" or "between" models.
 
The Fixed Effect (FE) model with first difference gets rid of the fixed effects (and its correlation with the explanatory variables). However, the error difference ($\Delta u_i = u_{i2} - u_{i1}$) may not be uncorrelated with all explanatory variables. The error term $u_{i1}$ is guaranteed to be uncorrelated to all $x_{i1}$ but may be correlated to $x_{i2}$. In our case, unobserved factors such as increased highway patrol may remain the same or vary across time. Hence, the first difference model, again, is not likely to give unbiased or consistent estimate. The model has $R^2 = 0.178$, which means that the model has left a lot of variance in the residuals, instead of accounting for in the explanatory variables.

The "within" model has $R^2 = 0.642$. This model, which includes the de-meaned values of the outcome and the explanatory variables, seems to perform well. The result of de-mean action removed the fixed effect variables and preserves the error terms uncorrelated characteristic with the explanatory variables. The residuals seem to approximate a normal distribution, shown in the QQ-Plot below. This model is probably a tad better than the pooled OLS but it is very close.

The "between" model has $R^2 = 0.822$ and does the best job of capturing most of the variance in the model. The QQ-Plot also shows adherence to a normal distribution. The number of observations are only 48, as expected. This model does a good job because the average of each state across 25 years are likely independent of each other. In general, observation across each state tend to be independent. By and large each state is autonomous in setting driving regulations.

Based on the pFtest result, p-value of 0.002243, we reject the null hypothesis of no fixed effects. This means we should include state and/or time fixed effects in our model. The QQ-Plot for the "between" model gives the best results along with the $R^2$ values. Thus, we choose the "between" model as the most optimal one.

Comparing both the Wooldridge's hypothesis tests, we see that the both the models (FD and FE) suffer from serial correlation. This suggests that both the models are not good and we need to fit a better model.

```{r Fixed Effect Tests}
pwfdtest(fd.model, data=traffic_df, index=c("state","year"), h0="fe")
pwfdtest(fd.model, data=traffic_df, index=c("state","year"), h0="fd")
pFtest(between.model,pool.model)
pFtest(within.model, pool.model)

par(mfrow=c(2,2))
qqnorm(pool.model$residuals)
qqline(pool.model$residuals)

qqnorm(fd.model$residuals)
qqline(fd.model$residuals)

qqnorm(within.model$residuals)
qqline(within.model$residuals)

qqnorm(between.model$residuals)
qqline(between.model$residuals)
```

# (10 points) Consider a Random Effects Model 

Instead of estimating a fixed effects model, should you have estimated a random effects model?

- Please state the assumptions of a random effects model, and evaluate whether these assumptions are met in the data. 
- If the assumptions are, in fact, met in the data, then estimate a random effects model and interpret the coefficients of this model. Comment on how, if at all, the estimates from this model have changed compared to the fixed effects model. 
- If the assumptions are **not** met, then do not estimate the data. But, also comment on what the consequences would be if you were to *inappropriately* estimate a random effects model. Would your coefficient estimates be biased or not? Would your standard error estimates be biased or not? Or, would there be some other problem that might arise?

We will start by setting the model and then testing the assumptions needed for the random effect model. 

```{r RE model start}
re.model <- plm(total_fatality_rate ~ year+bac10 + adm_lic_revoc_law + primary_seatbelt_law + 
                  secondary_seatbelt_law + sl75 + slnone + 
                  grad_drivers_lic_law + percent_pop_aged_14_to_24 + 
                  log(unemployment_rate) + log(miles_driven_per_capita), 
                data=traffic_df, index=c("state", "year"), model="random")
```

The first assumption of the random effects model is that there are no perfect linear relationships among the explanatory variables. 

```{r checks for linear relationship}
colin_data <- traffic_df
colin_data$logmiles_driven <- log(colin_data$miles_driven_per_capita)
colin_data$logunemp <- log(colin_data$unemployment_rate)

colin_data = colin_data %>% select(year,adm_lic_revoc_law, primary_seatbelt_law,secondary_seatbelt_law,sl75,slnone,grad_drivers_lic_law, 
                                   percent_pop_aged_14_to_24,logunemp, 
                                   logmiles_driven)
corrplot(cor(colin_data), method="color", number.cex=0.5, tl.cex=0.5, 
         type='lower')
```
**From the above correlation plot we see some concerning correlations, particularly between percent of population aged 14 to 24 and the year variable as well as the log of unemployment and the year. These high correlations may indicate multicollinearity (or if there is a perfect linear relationship between variables).**

**Additionally, we will examine the Variance Inflation Factor matrix of the model for multicollinearity.**

```{r vif for RE model}
car::vif(re.model)
```
**We see high values for year=(1995, 1996, 2000, 2003, 2004) percent_pop_aged_14_to_24, and log(miles_driven_per_capita) indicating the possible presence of multicollinearity in these variables.**

**Assessing our analysis of multicollinearity in the model explanatory variables, we have not provided sufficient evidence in supporting the lack of perfect multicollinearity between the variables as there are some strong correlations (linear relationships) between certain variables.**

**The second assumption is that there is no correlation between the unobserved effect (random and fixed effects) and the explanatory variables. Using a random effects model imposes the error structure that the error term** $v_{it}$ **is equal to the sum of variation between groups and variation within groups onto the model residuals, allowing to properly specify the residuals and more efficiently estimate the coefficients of interest. This requires the assumption of independence between random effects and the other predictors in the model. The assumptions for the fixed effect model are discussed above, the additional assumption of independence of random effects and other predictors in the model is evaluated below. The test we run is the Hausman Test for fixed versus random effects. The null hypothesis is that the random effects model is acceptable while the alternative hypothesis is that there is correlation between residuals and predictors, meaning that we should use the FE model.**

```{r model comparison}
phtest(between.model, re.model)
#traffic.new.row
```

>The Hausman test results in a p-value of `phtest(between.model, re.model)$p.value` which is far above the standard 0.05 value used to determine statistical significance. In this case, we do not have support to reject the null hypothesis and are thus can assume that we should accept the null hypothesis that the random effects model is acceptable as both models are consistent, and we know that the random effects model is more efficient, and thus the preferable model to use. 

>The third assumption is that of homoskedastic errors, which we can test below using the Pesaran's CD test: 

```{r pcdtest for RE}
pcdtest(re.model)
```
**While the p-value is not statistically significant indicating that we do not have support to reject the null hypothesis in favor of the alternative hypothesis of cross-sectional dependence, we have already violated assumption 2 and likely assumption 1, and thus should not proceed with this model.**

**Overall, the main assumption of the RE model, assumption 2, was violated by showing that the RE model is inconsistent. Additionally, there are concerns of multicollinearity in the data. From this, we will not proceed with estimating the random effects model.** 

**If we were to inappropriately estimate a random effects model, we would be incorrectly assuming that the random effects and other predictors are independent of one another. This would lead to omitted variable bias as the correlation between the random effects and the explanatory variables of interest would not allow for accurate estimation of the coefficient. Standard errors will also be biased as we are assuming that the random effects, which are included in the error term, are incorrectly uncorrelated with the predictors - given that there is correlation, this will introduce bias into the standard errors.**

# (10 points) Model Forecasts 
The COVID-19 pandemic dramatically changed patterns of driving. Find data (and include this data in your analysis, here) that includes some measure of vehicle miles driven in the US. Your data should at least cover the period from January 2018 to as current as possible. With this data, produce the following statements: 

- Comparing monthly miles driven in 2018 to the same months during the pandemic: 
  - What month demonstrated the largest decrease in driving? How much, in percentage terms, lower was this driving? 
  - What month demonstrated the largest increase in driving? How much, in percentage terms, higher was this driving? 
  
Now, use these changes in driving to make forecasts from your models. 

- Suppose that the number of miles driven per capita, increased by as much as the COVID boom. Using the FE estimates, what would the consequences be on the number of traffic fatalities? Please interpret the estimate.
- Suppose that the number of miles driven per capita, decreased by as much as the COVID bust. Using the FE estimates, what would the consequences be on the number of traffic fatalities? Please interpret the estimate.

```{r read new data, warning=FALSE}
setwd("/home/rstudio/kumarn/MIDS/w271/Lab-3")
vehicle_miles <- read.csv('./data/vehiclemiles.csv')
```

```{r new columns and format}
vehicle_miles$DATE <- as.Date(vehicle_miles$DATE)
vehicle_miles$year <- format(vehicle_miles$DATE, format="%Y")
head(vehicle_miles)
```

**We define pandemic months as the time frame between March 2020 (when COVID first hit) and March 2021 when the vaccine roll out had begun.**

```{r preliminary analysis}
drive_2018 <- vehicle_miles %>% filter(year == 2018)
drive_pandemic <- vehicle_miles %>% filter(year == 2020 | year == 2021)
drive_2018$month <- format(drive_2018$DATE, "%m")
drive_pandemic$month <- format(drive_pandemic$DATE, "%m")
drive_pandemic <- drive_pandemic %>% slice(3:14)
drive_pandemic$group <- 'pandemic'
drive_2018$group <- 2018
comparison <- rbind(drive_2018, drive_pandemic)
```

```{r forecast visauls}
ggplot(comparison, aes(x=month, y=TRFVOLUSM227NFWA, group=group)) + 
  geom_line(aes(color=group)) + ylab('Miles Driven') + 
  ggtitle('Miles Driven in 2018 vs Pandemic Months')
```

**Visually from the plot above, we see the biggest differences coming in April and May. We will confirm further below.**

```{r additional data}
drive_pandemic_month <- drive_pandemic %>% arrange(month)
differences <- drive_2018$TRFVOLUSM227NFWA - 
  drive_pandemic_month$TRFVOLUSM227NFWA
perc <- scales::percent(drive_pandemic_month$TRFVOLUSM227NFWA[4] / 
                          drive_2018$TRFVOLUSM227NFWA[4])
```

>We find the months with the largest differences to have been April, 2018 had `r differences[4]` more million miles driven. In percentage terms, in April 2020, Americans drove `r perc` the amount that they did in April of 2018.

```{r largest increase in driving}
may_driving_2021 <- max(diff(drive_pandemic$TRFVOLUSM227NFWA))
perc2 <- scales::percent(drive_pandemic$TRFVOLUSM227NFWA[4] / 
                           drive_pandemic$TRFVOLUSM227NFWA[3] - 1)
```

>The maximum increase in driving was from April 2020 to May 2020, where driving increased by `r may_driving_2021` millions of miles. This represents a `r perc2` increase month over month. 

>Now, use these changes in driving to make forecasts from the models.

```{r FE estimate on fatality with increase by COVID boom}
effect1 <- between.model$coefficients['log(miles_driven_per_capita)'] *
  (drive_pandemic$TRFVOLUSM227NFWA[4]/drive_pandemic$TRFVOLUSM227NFWA[3])
```

>If the number of miles driven per-capita increased by as much as the COVID boom, the consequences of traffic fatalities would be expected to be an increase of `r effect1` percent (in the log of the per-capita miles driven).

```{r FE estimate on fatality with decrese by COVID boom}
effect2 <- between.model$coefficients['log(miles_driven_per_capita)'] * 
  (-(1-drive_pandemic_month$TRFVOLUSM227NFWA[4]/drive_2018$TRFVOLUSM227NFWA[4]))
```
If the number of miles driven per capita increased by as much as the COVID boom, the consequences of traffic fatalities would be expected to be an increase of `r effect2` percent. 

# (5 points) Evaluate Error 

If there were serial correlation or heteroskedasticity in the idiosyncratic errors of the model, what would be the consequences on the estimators and their standard errors? Is there any serial correlation or heteroskedasticity? 

**Serial correlation or heteroskedasticity in the idiosyncratic errors will cause the standard errors for the model to be incorrect. CLT can be invoked for first-difference models (Wooldridge). However, for fixed effects models, this is not necessarily the case as we'll likely see AR(1) effects in the error terms. Unless the AR(1) is a random walk we'll have to account for the AR(1) correlation. In case of random walk the first difference will be white noise and thus can be worked around. Let us use the Breusch-Godfrey test to check for serial correlation.**

```{r pbg test for serial correlation}
pbgtest(pool.model)
pbgtest(fd.model)
pbgtest(within.model)
pbgtest(between.model)
```

**The test reveals for the serial correlation of the "between model" is the only one to not show serial correlation. The p-value does not reject the null hypothesis of no serial correlation. Thus the computed standard errors is likely to not have serial correlation.**

**Let us now look at the serial correlation of the residuals** 

```{r plot model resids}
acf(between.model$residuals)
```

**The ACF plot confirms that the correlation is of significance only at lag 0, and thus no serial correlation.** 

```{r acf for other models, include=FALSE}
acf(pool.model$residuals)
acf(fd.model$residuals)
acf(within.model$residuals)
```
